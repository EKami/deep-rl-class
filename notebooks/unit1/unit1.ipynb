{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"view-in-github\",\n",
    "        \"colab_type\": \"text\"\n",
    "      },\n",
    "      \"source\": [\n",
    "        \"<a href=\\\"https://colab.research.google.com/github/EKami/deep-rl-class/blob/feature%2Fpersonal-branch/notebooks/unit1/unit1.ipynb\\\" target=\\\"_parent\\\"><img src=\\\"https://colab.research.google.com/assets/colab-badge.svg\\\" alt=\\\"Open In Colab\\\"/></a>\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"markdown\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"njb_ProuHiOe\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Unit 1: Train your first Deep Reinforcement Learning Agent ü§ñ\\n\",\n",
    "    \"\\n\",\n",
    "    \"![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, you'll train your **first Deep Reinforcement Learning agent** a Lunar Lander agent that will learn to **land correctly on the Moon üåï**. Using [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) a Deep Reinforcement Learning library, share them with the community, and experiment with different configurations\\n\",\n",
    "    \"\\n\",\n",
    "    \"‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.** ‚¨áÔ∏è\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"PF46MwbZD00b\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<video controls autoplay><source src=\\\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\\\" type=\\\"video/mp4\\\"></video>\\n\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%html\\n\",\n",
    "    \"<video controls autoplay><source src=\\\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\\\" type=\\\"video/mp4\\\"></video>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"x7oR6R-ZIbeS\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"### The environment üéÆ\\n\",\n",
    "    \"\\n\",\n",
    "    \"- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\\n\",\n",
    "    \"\\n\",\n",
    "    \"### The library used üìö\\n\",\n",
    "    \"\\n\",\n",
    "    \"- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"OwEcFHe9RRZW\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"4i6tjI2tHQ8j\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Objectives of this notebook üèÜ\\n\",\n",
    "    \"\\n\",\n",
    "    \"At the end of the notebook, you will:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Be able to use **Gymnasium**, the environment library.\\n\",\n",
    "    \"- Be able to use **Stable-Baselines3**, the deep reinforcement learning library.\\n\",\n",
    "    \"- Be able to **push your trained agent to the Hub** with a nice video replay and an evaluation score üî•.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Ff-nyJdzJPND\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## This notebook is from Deep Reinforcement Learning Course\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\\\" alt=\\\"Deep RL Course illustration\\\"/>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"6p5HnEefISCB\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"In this free course, you will:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- üìñ Study Deep Reinforcement Learning in **theory and practice**.\\n\",\n",
    "    \"- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\\n\",\n",
    "    \"- ü§ñ Train **agents in unique environments**\\n\",\n",
    "    \"- üéì **Earn a certificate of completion** by completing 80% of the assignments.\\n\",\n",
    "    \"\\n\",\n",
    "    \"And more!\\n\",\n",
    "    \"\\n\",\n",
    "    \"Check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\\n\",\n",
    "    \"\\n\",\n",
    "    \"Don‚Äôt forget to **<a href=\\\"http://eepurl.com/ic5ZUD\\\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\\n\",\n",
    "    \"\\n\",\n",
    "    \"The best way to keep in touch and ask questions is **to join our discord server** to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Y-mo_6rXIjRi\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Prerequisites üèóÔ∏è\\n\",\n",
    "    \"\\n\",\n",
    "    \"Before diving into the notebook, you need to:\\n\",\n",
    "    \"\\n\",\n",
    "    \"üî≤ üìù **[Read Unit 0](https://huggingface.co/deep-rl-course/unit0/introduction)** that gives you all the **information about the course and helps you to onboard** ü§ó\\n\",\n",
    "    \"\\n\",\n",
    "    \"üî≤ üìö **Develop an understanding of the foundations of Reinforcement learning** (RL process, Rewards hypothesis...) by [reading Unit 1](https://huggingface.co/deep-rl-course/unit1/introduction).\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"HoeqMnr5LuYE\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## A small recap of Deep Reinforcement Learning üìö\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\\\" alt=\\\"The RL process\\\" width=\\\"100%\\\">\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"xcQYx9ynaFMD\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Let's do a small recap on what we learned in the first Unit:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Reinforcement Learning is a **computational approach to learning from actions**. We build an agent that learns from the environment by **interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- The goal of any RL agent is to **maximize its expected cumulative reward** (also called expected return) because RL is based on the _reward hypothesis_, which is that all goals can be described as the maximization of an expected cumulative reward.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- The RL process is a **loop that outputs a sequence of state, action, reward, and next state**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- To calculate the expected cumulative reward (expected return), **we discount the rewards**: the rewards that come sooner (at the beginning of the game) are more probable to happen since they are more predictable than the long-term future reward.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- To solve an RL problem, you want to **find an optimal policy**; the policy is the \\\"brain\\\" of your AI that will tell us what action to take given a state. The optimal one is the one that gives you the actions that max the expected return.\\n\",\n",
    "    \"\\n\",\n",
    "    \"There are **two** ways to find your optimal policy:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- By **training your policy directly**: policy-based methods.\\n\",\n",
    "    \"- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name \\\"deep.\\\"**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"qDploC3jSH99\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"# Let's train our first Deep Reinforcement Learning agent and upload it to the Hub üöÄ\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Get a certificate üéì\\n\",\n",
    "    \"\\n\",\n",
    "    \"To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained model to the Hub and **get a result of >= 200**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) and find your model, **the result = mean_reward - std of reward**\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"HqzznTzhNfAC\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Set the GPU üí™\\n\",\n",
    "    \"\\n\",\n",
    "    \"- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\\\" alt=\\\"GPU Step 1\\\">\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"38HBd3t1SHJ8\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"- `Hardware Accelerator > GPU`\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\\\" alt=\\\"GPU Step 2\\\">\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"jeDAH0h0EBiG\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Install dependencies and create a virtual screen üîΩ\\n\",\n",
    "    \"\\n\",\n",
    "    \"The first step is to install the dependencies, we‚Äôll install multiple ones.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- `gymnasium[box2d]`: Contains the LunarLander-v2 environment üåõ\\n\",\n",
    "    \"- `stable-baselines3[extra]`: The deep reinforcement learning library.\\n\",\n",
    "    \"- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To make things easier, we created a script to install all these dependencies.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BEKeXQJsQCYm\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\\n\",\n",
    "    \"\\n\",\n",
    "    \"Hence the following cell will install virtual screen libraries and create and run a virtual screen üñ•\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"TCwBTAwAW9JJ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"To make sure the new installed libraries are used, **sometimes it's required to restart the notebook runtime**. The next cell will force the **runtime to crash, so you'll need to connect again and run the code starting from here**. Thanks to this trick, **we will be able to run our virtual screen.**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BE5JWP5rQIKf\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"<pyvirtualdisplay.display.Display at 0x7bb13f382f10>\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 1,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# Virtual display\\n\",\n",
    "    \"from pyvirtualdisplay import Display\\n\",\n",
    "    \"\\n\",\n",
    "    \"virtual_display = Display(visible=0, size=(1400, 900))\\n\",\n",
    "    \"virtual_display.start()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"wrgpVFqyENVf\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Import the packages üì¶\\n\",\n",
    "    \"\\n\",\n",
    "    \"One additional library we import is huggingface_hub **to be able to upload and download trained models from the hub**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"The Hugging Face Hub ü§ó works as a central place where anyone can share and explore models and datasets. It has versioning, metrics, visualizations and other features that will allow you to easily collaborate with others.\\n\",\n",
    "    \"\\n\",\n",
    "    \"You can see here all the Deep reinforcement Learning models available hereüëâ https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"cygWLPGsEQ0m\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import gymnasium\\n\",\n",
    "    \"\\n\",\n",
    "    \"from huggingface_sb3 import load_from_hub, package_to_hub\\n\",\n",
    "    \"from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\\n\",\n",
    "    \"\\n\",\n",
    "    \"from stable_baselines3 import PPO\\n\",\n",
    "    \"from stable_baselines3.common.env_util import make_vec_env\\n\",\n",
    "    \"from stable_baselines3.common.evaluation import evaluate_policy\\n\",\n",
    "    \"from stable_baselines3.common.monitor import Monitor\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MRqRuRUl8CsB\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Understand Gymnasium and how it works ü§ñ\\n\",\n",
    "    \"\\n\",\n",
    "    \"üèã The library containing our environment is called Gymnasium.\\n\",\n",
    "    \"**You'll use Gymnasium a lot in Deep Reinforcement Learning.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"Gymnasium is the **new version of Gym library** [maintained by the Farama Foundation](https://farama.org/).\\n\",\n",
    "    \"\\n\",\n",
    "    \"The Gymnasium library provides two things:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- An interface that allows you to **create RL environments**.\\n\",\n",
    "    \"- A **collection of environments** (gym-control, atari, box2D...).\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's look at an example, but first let's recall the RL loop.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\\\" alt=\\\"The RL process\\\" width=\\\"100%\\\">\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"-TzNN0bQ_j-3\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"At each step:\\n\",\n",
    "    \"- Our Agent receives¬†a **state (S0)**¬†from the¬†**Environment**¬†‚Äî we receive the first frame of our game (Environment).\\n\",\n",
    "    \"- Based on that¬†**state (S0),**¬†the Agent takes an¬†**action (A0)**¬†‚Äî our Agent will move to the right.\\n\",\n",
    "    \"- The environment transitions to a¬†**new**¬†**state (S1)**¬†‚Äî new frame.\\n\",\n",
    "    \"- The environment gives some¬†**reward (R1)**¬†to the Agent ‚Äî we‚Äôre not dead¬†*(Positive Reward +1)*.\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"With Gymnasium:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1Ô∏è‚É£ We create our environment using `gymnasium.make()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"2Ô∏è‚É£ We reset the environment to its initial state with `observation = env.reset()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"At each step:\\n\",\n",
    "    \"\\n\",\n",
    "    \"3Ô∏è‚É£ Get an action using our model (in our example we take a random action)\\n\",\n",
    "    \"\\n\",\n",
    "    \"4Ô∏è‚É£ Using `env.step(action)`, we perform this action in the environment and get\\n\",\n",
    "    \"- `observation`: The new state (st+1)\\n\",\n",
    "    \"- `reward`: The reward we get after executing the action\\n\",\n",
    "    \"- `terminated`: Indicates if the episode terminated (agent reach the terminal state)\\n\",\n",
    "    \"- `truncated`: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\\n\",\n",
    "    \"- `info`: A dictionary that provides additional information (depends on the environment).\\n\",\n",
    "    \"\\n\",\n",
    "    \"For more explanations check this üëâ https://gymnasium.farama.org/api/env/#gymnasium.Env.step\\n\",\n",
    "    \"\\n\",\n",
    "    \"If the episode is terminated:\\n\",\n",
    "    \"- We reset the environment to its initial state with `observation = env.reset()`\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Let's look at an example!** Make sure to read the code\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"w7vOFlpA_ONz\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Action taken: 2\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 2\\n\",\n",
    "      \"Action taken: 2\\n\",\n",
    "      \"Action taken: 3\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 3\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 1\\n\",\n",
    "      \"Action taken: 2\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 3\\n\",\n",
    "      \"Action taken: 3\\n\",\n",
    "      \"Action taken: 1\\n\",\n",
    "      \"Action taken: 0\\n\",\n",
    "      \"Action taken: 3\\n\",\n",
    "      \"Action taken: 2\\n\",\n",
    "      \"Action taken: 1\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"\\n\",\n",
    "    \"# First, we create our environment called LunarLander-v2\\n\",\n",
    "    \"env = gym.make(\\\"LunarLander-v2\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Then we reset this environment\\n\",\n",
    "    \"observation, info = env.reset()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for _ in range(20):\\n\",\n",
    "    \"  # Take a random action\\n\",\n",
    "    \"  action = env.action_space.sample()\\n\",\n",
    "    \"  print(\\\"Action taken:\\\", action)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  # Do this action in the environment and get\\n\",\n",
    "    \"  # next_state, reward, terminated, truncated and info\\n\",\n",
    "    \"  observation, reward, terminated, truncated, info = env.step(action)\\n\",\n",
    "    \"\\n\",\n",
    "    \"  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\\n\",\n",
    "    \"  if terminated or truncated:\\n\",\n",
    "    \"      # Reset the environment\\n\",\n",
    "    \"      print(\\\"Environment is reset\\\")\\n\",\n",
    "    \"      observation, info = env.reset()\\n\",\n",
    "    \"\\n\",\n",
    "    \"env.close()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"XIrKGGSlENZB\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Create the LunarLander environment üåõ and understand how it works\\n\",\n",
    "    \"\\n\",\n",
    "    \"### [The environment üéÆ](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this first tutorial, we‚Äôre going to train our agent, a [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/), **to land correctly on the moon**. To do that, the agent needs to learn **to adapt its speed and position (horizontal, vertical, and angular) to land correctly.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"üí° A good habit when you start to use an environment is to check its documentation\\n\",\n",
    "    \"\\n\",\n",
    "    \"üëâ https://gymnasium.farama.org/environments/box2d/lunar_lander/\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"poLBgRocF9aT\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Let's see what the Environment looks like:\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ZNPG0g_UGCfh\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"_____OBSERVATION SPACE_____ \\n\",\n",
    "      \"\\n\",\n",
    "      \"Observation Space Shape (8,)\\n\",\n",
    "      \"Sample observation [ 57.940533   -49.733433    -3.9973917   -1.3286986   -2.0896318\\n\",\n",
    "      \"  -1.5724549    0.20104052   0.33471182]\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# We create our environment with gym.make(\\\"<name_of_the_environment>\\\")\\n\",\n",
    "    \"env = gym.make(\\\"LunarLander-v2\\\")\\n\",\n",
    "    \"env.reset()\\n\",\n",
    "    \"print(\\\"_____OBSERVATION SPACE_____ \\\\n\\\")\\n\",\n",
    "    \"print(\\\"Observation Space Shape\\\", env.observation_space.shape)\\n\",\n",
    "    \"print(\\\"Sample observation\\\", env.observation_space.sample()) # Get a random observation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"2MXc15qFE0M9\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"We see with `Observation Space Shape (8,)` that the observation is a vector of size 8, where each value contains different information about the lander:\\n\",\n",
    "    \"- Horizontal pad coordinate (x)\\n\",\n",
    "    \"- Vertical pad coordinate (y)\\n\",\n",
    "    \"- Horizontal speed (x)\\n\",\n",
    "    \"- Vertical speed (y)\\n\",\n",
    "    \"- Angle\\n\",\n",
    "    \"- Angular speed\\n\",\n",
    "    \"- If the left leg contact point has touched the land (boolean)\\n\",\n",
    "    \"- If the right leg contact point has touched the land (boolean)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"We5WqOBGLoSm\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\n\",\n",
    "      \" _____ACTION SPACE_____ \\n\",\n",
    "      \"\\n\",\n",
    "      \"Action Space Shape 4\\n\",\n",
    "      \"Action Space Sample 2\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(\\\"\\\\n _____ACTION SPACE_____ \\\\n\\\")\\n\",\n",
    "    \"print(\\\"Action Space Shape\\\", env.action_space.n)\\n\",\n",
    "    \"print(\\\"Action Space Sample\\\", env.action_space.sample()) # Take a random action\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"MyxXwkI2Magx\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"The action space (the set of possible actions the agent can take) is discrete with 4 actions available üéÆ:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Action 0: Do nothing,\\n\",\n",
    "    \"- Action 1: Fire left orientation engine,\\n\",\n",
    "    \"- Action 2: Fire the main engine,\\n\",\n",
    "    \"- Action 3: Fire right orientation engine.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Reward function (the function that will give a reward at each timestep) üí∞:\\n\",\n",
    "    \"\\n\",\n",
    "    \"After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"For each step, the reward:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Is increased/decreased the closer/further the lander is to the landing pad.\\n\",\n",
    "    \"-  Is increased/decreased the slower/faster the lander is moving.\\n\",\n",
    "    \"- Is decreased the more the lander is tilted (angle not horizontal).\\n\",\n",
    "    \"- Is increased by 10 points for each leg that is in contact with the ground.\\n\",\n",
    "    \"- Is decreased by 0.03 points each frame a side engine is firing.\\n\",\n",
    "    \"- Is decreased by 0.3 points each frame the main engine is firing.\\n\",\n",
    "    \"\\n\",\n",
    "    \"The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"An episode is **considered a solution if it scores at least 200 points.**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"dFD9RAFjG8aq\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"#### Vectorized Environment\\n\",\n",
    "    \"\\n\",\n",
    "    \"- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"99hqQ_etEy1N\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create the environment\\n\",\n",
    "    \"env = make_vec_env('LunarLander-v2', n_envs=16)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"VgrE86r5E5IK\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Create the Model ü§ñ\\n\",\n",
    "    \"- We have studied our environment and we understood the problem: **being able to land the Lunar Lander to the Landing Pad correctly by controlling left, right and main orientation engine**. Now let's build the algorithm we're going to use to solve this Problem üöÄ.\\n\",\n",
    "    \"\\n\",\n",
    "    \"- To do so, we're going to use our first Deep RL library, [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/).\\n\",\n",
    "    \"\\n\",\n",
    "    \"- SB3 is a set of **reliable implementations of reinforcement learning algorithms in PyTorch**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"üí° A good habit when using a new library is to dive first on the documentation: https://stable-baselines3.readthedocs.io/en/master/ and then try some tutorials.\\n\",\n",
    "    \"\\n\",\n",
    "    \"----\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"HLlClRW37Q7e\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\\\" alt=\\\"Stable Baselines3\\\">\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"HV4yiUM_9_Ka\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"To solve this problem, we're going to use SB3 **PPO**. [PPO (aka Proximal Policy Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning algorithms that you'll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).\\n\",\n",
    "    \"\\n\",\n",
    "    \"PPO is a combination of:\\n\",\n",
    "    \"- *Value-based reinforcement learning method*: learning an action-value function that will tell us the **most valuable action to take given a state and action**.\\n\",\n",
    "    \"- *Policy-based reinforcement learning method*: learning a policy that will **give us a probability distribution over actions**.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"5qL_4HeIOrEJ\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Stable-Baselines3 is easy to set up:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1Ô∏è‚É£ You **create your environment** (in our case it was done above)\\n\",\n",
    "    \"\\n\",\n",
    "    \"2Ô∏è‚É£ You define the **model you want to use and instantiate this model** `model = PPO(\\\"MlpPolicy\\\")`\\n\",\n",
    "    \"\\n\",\n",
    "    \"3Ô∏è‚É£ You **train the agent** with `model.learn` and define the number of training timesteps\\n\",\n",
    "    \"\\n\",\n",
    "    \"```\\n\",\n",
    "    \"# Create environment\\n\",\n",
    "    \"env = gym.make('LunarLander-v2')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Instantiate the agent\\n\",\n",
    "    \"model = PPO('MlpPolicy', env, verbose=1)\\n\",\n",
    "    \"# Train the agent\\n\",\n",
    "    \"model.learn(total_timesteps=int(2e5))\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"nxI6hT1GE4-A\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO: Define a PPO MlpPolicy architecture\\n\",\n",
    "    \"# We use MultiLayerPerceptron (MLPPolicy) because the input is a vector,\\n\",\n",
    "    \"# if we had frames as input we would use CnnPolicy\\n\",\n",
    "    \"#model = PPO(\\n\",\n",
    "        \"    'MlpPolicy',\\n\",\n",
    "        \"    env,\\n\",\n",
    "        \"    verbose=1,\\n\",\n",
    "        \"\\n\",\n",
    "        \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"QAN7B0_HCVZC\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"#### Solution\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"543OHYDfcjK4\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Using cuda device\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# SOLUTION\\n\",\n",
    "    \"# We added some parameters to accelerate the training\\n\",\n",
    "    \"model = PPO(\\n\",\n",
    "    \"    policy = 'MlpPolicy',\\n\",\n",
    "    \"    env = env,\\n\",\n",
    "    \"    n_steps = 1024,\\n\",\n",
    "    \"    batch_size = 64,\\n\",\n",
    "    \"    n_epochs = 4,\\n\",\n",
    "    \"    gamma = 0.999,\\n\",\n",
    "    \"    gae_lambda = 0.98,\\n\",\n",
    "    \"    ent_coef = 0.01,\\n\",\n",
    "    \"    verbose=1)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"ClJJk88yoBUi\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Train the PPO agent üèÉ\\n\",\n",
    "    \"- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~20min, but you can use fewer timesteps if you just want to try it out.\\n\",\n",
    "    \"- During the training, take a ‚òï break you deserved it ü§ó\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"qKnYkNiVp89p\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO: Train it for 1,000,000 timesteps\\n\",\n",
    "    \"\\n\",\n",
    "    \"# TODO: Specify file name for model and save the model to file\\n\",\n",
    "    \"#model_name = \\\"ppo-LunarLander-v2\\\"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"1bQzQ-QcE3zo\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"#### Solution\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"poBCy9u_csyR\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"---------------------------------\\n\",\n",
    "      \"| rollout/           |          |\\n\",\n",
    "      \"|    ep_len_mean     | 89.9     |\\n\",\n",
    "      \"|    ep_rew_mean     | -167     |\\n\",\n",
    "      \"| time/              |          |\\n\",\n",
    "      \"|    fps             | 10118    |\\n\",\n",
    "      \"|    iterations      | 1        |\\n\",\n",
    "      \"|    time_elapsed    | 1        |\\n\",\n",
    "      \"|    total_timesteps | 16384    |\\n\",\n",
    "      \"---------------------------------\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"| rollout/                |               |\\n\",\n",
    "      \"|    ep_len_mean          | 86.2          |\\n\",\n",
    "      \"|    ep_rew_mean          | -139          |\\n\",\n",
    "      \"| time/                   |               |\\n\",\n",
    "      \"|    fps                  | 7119          |\\n\",\n",
    "      \"|    iterations           | 2             |\\n\",\n",
    "      \"|    time_elapsed         | 4             |\\n\",\n",
    "      \"|    total_timesteps      | 32768         |\\n\",\n",
    "      \"| train/                  |               |\\n\",\n",
    "      \"|    approx_kl            | 0.0064969156  |\\n\",\n",
    "      \"|    clip_fraction        | 0.0426        |\\n\",\n",
    "      \"|    clip_range           | 0.2           |\\n\",\n",
    "      \"|    entropy_loss         | -1.38         |\\n\",\n",
    "      \"|    explained_variance   | -0.0017882586 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003        |\\n\",\n",
    "      \"|    loss                 | 1.19e+03      |\\n\",\n",
    "      \"|    n_updates            | 4             |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00522      |\\n\",\n",
    "      \"|    value_loss           | 4.3e+03       |\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 91.3        |\\n\",\n",
    "      \"|    ep_rew_mean          | -131        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 6681        |\\n\",\n",
    "      \"|    iterations           | 3           |\\n\",\n",
    "      \"|    time_elapsed         | 7           |\\n\",\n",
    "      \"|    total_timesteps      | 49152       |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.006193991 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0376      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.37       |\\n\",\n",
    "      \"|    explained_variance   | 0.005393207 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 651         |\\n\",\n",
    "      \"|    n_updates            | 8           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00554    |\\n\",\n",
    "      \"|    value_loss           | 2.08e+03    |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 101          |\\n\",\n",
    "      \"|    ep_rew_mean          | -119         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 6462         |\\n\",\n",
    "      \"|    iterations           | 4            |\\n\",\n",
    "      \"|    time_elapsed         | 10           |\\n\",\n",
    "      \"|    total_timesteps      | 65536        |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.007807805  |\\n\",\n",
    "      \"|    clip_fraction        | 0.0422       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.37        |\\n\",\n",
    "      \"|    explained_variance   | -0.008426547 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 387          |\\n\",\n",
    "      \"|    n_updates            | 12           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00395     |\\n\",\n",
    "      \"|    value_loss           | 1.06e+03     |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"| rollout/                |                |\\n\",\n",
    "      \"|    ep_len_mean          | 104            |\\n\",\n",
    "      \"|    ep_rew_mean          | -103           |\\n\",\n",
    "      \"| time/                   |                |\\n\",\n",
    "      \"|    fps                  | 6327           |\\n\",\n",
    "      \"|    iterations           | 5              |\\n\",\n",
    "      \"|    time_elapsed         | 12             |\\n\",\n",
    "      \"|    total_timesteps      | 81920          |\\n\",\n",
    "      \"| train/                  |                |\\n\",\n",
    "      \"|    approx_kl            | 0.006616656    |\\n\",\n",
    "      \"|    clip_fraction        | 0.0407         |\\n\",\n",
    "      \"|    clip_range           | 0.2            |\\n\",\n",
    "      \"|    entropy_loss         | -1.35          |\\n\",\n",
    "      \"|    explained_variance   | -0.00051116943 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003         |\\n\",\n",
    "      \"|    loss                 | 473            |\\n\",\n",
    "      \"|    n_updates            | 16             |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00226       |\\n\",\n",
    "      \"|    value_loss           | 819            |\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"| rollout/                |                |\\n\",\n",
    "      \"|    ep_len_mean          | 103            |\\n\",\n",
    "      \"|    ep_rew_mean          | -81.2          |\\n\",\n",
    "      \"| time/                   |                |\\n\",\n",
    "      \"|    fps                  | 6282           |\\n\",\n",
    "      \"|    iterations           | 6              |\\n\",\n",
    "      \"|    time_elapsed         | 15             |\\n\",\n",
    "      \"|    total_timesteps      | 98304          |\\n\",\n",
    "      \"| train/                  |                |\\n\",\n",
    "      \"|    approx_kl            | 0.007180809    |\\n\",\n",
    "      \"|    clip_fraction        | 0.0829         |\\n\",\n",
    "      \"|    clip_range           | 0.2            |\\n\",\n",
    "      \"|    entropy_loss         | -1.34          |\\n\",\n",
    "      \"|    explained_variance   | -0.00015377998 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003         |\\n\",\n",
    "      \"|    loss                 | 234            |\\n\",\n",
    "      \"|    n_updates            | 20             |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00403       |\\n\",\n",
    "      \"|    value_loss           | 775            |\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"| rollout/                |                |\\n\",\n",
    "      \"|    ep_len_mean          | 106            |\\n\",\n",
    "      \"|    ep_rew_mean          | -60.1          |\\n\",\n",
    "      \"| time/                   |                |\\n\",\n",
    "      \"|    fps                  | 6242           |\\n\",\n",
    "      \"|    iterations           | 7              |\\n\",\n",
    "      \"|    time_elapsed         | 18             |\\n\",\n",
    "      \"|    total_timesteps      | 114688         |\\n\",\n",
    "      \"| train/                  |                |\\n\",\n",
    "      \"|    approx_kl            | 0.0072590625   |\\n\",\n",
    "      \"|    clip_fraction        | 0.0534         |\\n\",\n",
    "      \"|    clip_range           | 0.2            |\\n\",\n",
    "      \"|    entropy_loss         | -1.3           |\\n\",\n",
    "      \"|    explained_variance   | -0.00012362003 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003         |\\n\",\n",
    "      \"|    loss                 | 301            |\\n\",\n",
    "      \"|    n_updates            | 24             |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00571       |\\n\",\n",
    "      \"|    value_loss           | 501            |\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"---------------------------------------------\\n\",\n",
    "      \"| rollout/                |                 |\\n\",\n",
    "      \"|    ep_len_mean          | 114             |\\n\",\n",
    "      \"|    ep_rew_mean          | -38.6           |\\n\",\n",
    "      \"| time/                   |                 |\\n\",\n",
    "      \"|    fps                  | 6097            |\\n\",\n",
    "      \"|    iterations           | 8               |\\n\",\n",
    "      \"|    time_elapsed         | 21              |\\n\",\n",
    "      \"|    total_timesteps      | 131072          |\\n\",\n",
    "      \"| train/                  |                 |\\n\",\n",
    "      \"|    approx_kl            | 0.009338851     |\\n\",\n",
    "      \"|    clip_fraction        | 0.068           |\\n\",\n",
    "      \"|    clip_range           | 0.2             |\\n\",\n",
    "      \"|    entropy_loss         | -1.26           |\\n\",\n",
    "      \"|    explained_variance   | -1.04904175e-05 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003          |\\n\",\n",
    "      \"|    loss                 | 238             |\\n\",\n",
    "      \"|    n_updates            | 28              |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00615        |\\n\",\n",
    "      \"|    value_loss           | 413             |\\n\",\n",
    "      \"---------------------------------------------\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"| rollout/                |                |\\n\",\n",
    "      \"|    ep_len_mean          | 126            |\\n\",\n",
    "      \"|    ep_rew_mean          | -24            |\\n\",\n",
    "      \"| time/                   |                |\\n\",\n",
    "      \"|    fps                  | 5998           |\\n\",\n",
    "      \"|    iterations           | 9              |\\n\",\n",
    "      \"|    time_elapsed         | 24             |\\n\",\n",
    "      \"|    total_timesteps      | 147456         |\\n\",\n",
    "      \"| train/                  |                |\\n\",\n",
    "      \"|    approx_kl            | 0.0090487255   |\\n\",\n",
    "      \"|    clip_fraction        | 0.0559         |\\n\",\n",
    "      \"|    clip_range           | 0.2            |\\n\",\n",
    "      \"|    entropy_loss         | -1.21          |\\n\",\n",
    "      \"|    explained_variance   | -7.5101852e-06 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003         |\\n\",\n",
    "      \"|    loss                 | 179            |\\n\",\n",
    "      \"|    n_updates            | 32             |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00404       |\\n\",\n",
    "      \"|    value_loss           | 494            |\\n\",\n",
    "      \"--------------------------------------------\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"| rollout/                |               |\\n\",\n",
    "      \"|    ep_len_mean          | 157           |\\n\",\n",
    "      \"|    ep_rew_mean          | -34.2         |\\n\",\n",
    "      \"| time/                   |               |\\n\",\n",
    "      \"|    fps                  | 5667          |\\n\",\n",
    "      \"|    iterations           | 10            |\\n\",\n",
    "      \"|    time_elapsed         | 28            |\\n\",\n",
    "      \"|    total_timesteps      | 163840        |\\n\",\n",
    "      \"| train/                  |               |\\n\",\n",
    "      \"|    approx_kl            | 0.005770837   |\\n\",\n",
    "      \"|    clip_fraction        | 0.0355        |\\n\",\n",
    "      \"|    clip_range           | 0.2           |\\n\",\n",
    "      \"|    entropy_loss         | -1.19         |\\n\",\n",
    "      \"|    explained_variance   | -5.841255e-06 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003        |\\n\",\n",
    "      \"|    loss                 | 194           |\\n\",\n",
    "      \"|    n_updates            | 36            |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00261      |\\n\",\n",
    "      \"|    value_loss           | 544           |\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"| rollout/                |               |\\n\",\n",
    "      \"|    ep_len_mean          | 175           |\\n\",\n",
    "      \"|    ep_rew_mean          | -13.1         |\\n\",\n",
    "      \"| time/                   |               |\\n\",\n",
    "      \"|    fps                  | 5333          |\\n\",\n",
    "      \"|    iterations           | 11            |\\n\",\n",
    "      \"|    time_elapsed         | 33            |\\n\",\n",
    "      \"|    total_timesteps      | 180224        |\\n\",\n",
    "      \"| train/                  |               |\\n\",\n",
    "      \"|    approx_kl            | 0.008204968   |\\n\",\n",
    "      \"|    clip_fraction        | 0.0825        |\\n\",\n",
    "      \"|    clip_range           | 0.2           |\\n\",\n",
    "      \"|    entropy_loss         | -1.2          |\\n\",\n",
    "      \"|    explained_variance   | 3.2186508e-06 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003        |\\n\",\n",
    "      \"|    loss                 | 460           |\\n\",\n",
    "      \"|    n_updates            | 40            |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.0035       |\\n\",\n",
    "      \"|    value_loss           | 720           |\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"| rollout/                |               |\\n\",\n",
    "      \"|    ep_len_mean          | 238           |\\n\",\n",
    "      \"|    ep_rew_mean          | -16.8         |\\n\",\n",
    "      \"| time/                   |               |\\n\",\n",
    "      \"|    fps                  | 4813          |\\n\",\n",
    "      \"|    iterations           | 12            |\\n\",\n",
    "      \"|    time_elapsed         | 40            |\\n\",\n",
    "      \"|    total_timesteps      | 196608        |\\n\",\n",
    "      \"| train/                  |               |\\n\",\n",
    "      \"|    approx_kl            | 0.005472468   |\\n\",\n",
    "      \"|    clip_fraction        | 0.0374        |\\n\",\n",
    "      \"|    clip_range           | 0.2           |\\n\",\n",
    "      \"|    entropy_loss         | -1.18         |\\n\",\n",
    "      \"|    explained_variance   | 3.3915043e-05 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003        |\\n\",\n",
    "      \"|    loss                 | 216           |\\n\",\n",
    "      \"|    n_updates            | 44            |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00198      |\\n\",\n",
    "      \"|    value_loss           | 513           |\\n\",\n",
    "      \"-------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 318          |\\n\",\n",
    "      \"|    ep_rew_mean          | -11.5        |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 4385         |\\n\",\n",
    "      \"|    iterations           | 13           |\\n\",\n",
    "      \"|    time_elapsed         | 48           |\\n\",\n",
    "      \"|    total_timesteps      | 212992       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.004707109  |\\n\",\n",
    "      \"|    clip_fraction        | 0.0233       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.21        |\\n\",\n",
    "      \"|    explained_variance   | 0.0040996075 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 270          |\\n\",\n",
    "      \"|    n_updates            | 48           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00178     |\\n\",\n",
    "      \"|    value_loss           | 545          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 423          |\\n\",\n",
    "      \"|    ep_rew_mean          | -1.89        |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 4091         |\\n\",\n",
    "      \"|    iterations           | 14           |\\n\",\n",
    "      \"|    time_elapsed         | 56           |\\n\",\n",
    "      \"|    total_timesteps      | 229376       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0064799143 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0554       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.23        |\\n\",\n",
    "      \"|    explained_variance   | 0.0025967956 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 231          |\\n\",\n",
    "      \"|    n_updates            | 52           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00165     |\\n\",\n",
    "      \"|    value_loss           | 411          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 403         |\\n\",\n",
    "      \"|    ep_rew_mean          | 10.1        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 3925        |\\n\",\n",
    "      \"|    iterations           | 15          |\\n\",\n",
    "      \"|    time_elapsed         | 62          |\\n\",\n",
    "      \"|    total_timesteps      | 245760      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.005157946 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0262      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.22       |\\n\",\n",
    "      \"|    explained_variance   | 0.26830518  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 75.4        |\\n\",\n",
    "      \"|    n_updates            | 56          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00261    |\\n\",\n",
    "      \"|    value_loss           | 309         |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 397         |\\n\",\n",
    "      \"|    ep_rew_mean          | 15.9        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 3737        |\\n\",\n",
    "      \"|    iterations           | 16          |\\n\",\n",
    "      \"|    time_elapsed         | 70          |\\n\",\n",
    "      \"|    total_timesteps      | 262144      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003440478 |\\n\",\n",
    "      \"|    clip_fraction        | 0.00836     |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.19       |\\n\",\n",
    "      \"|    explained_variance   | 0.40273744  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 281         |\\n\",\n",
    "      \"|    n_updates            | 60          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00108    |\\n\",\n",
    "      \"|    value_loss           | 422         |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 451          |\\n\",\n",
    "      \"|    ep_rew_mean          | 23.3         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 3570         |\\n\",\n",
    "      \"|    iterations           | 17           |\\n\",\n",
    "      \"|    time_elapsed         | 78           |\\n\",\n",
    "      \"|    total_timesteps      | 278528       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0048448034 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0235       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.21        |\\n\",\n",
    "      \"|    explained_variance   | 0.62165534   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 117          |\\n\",\n",
    "      \"|    n_updates            | 64           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00159     |\\n\",\n",
    "      \"|    value_loss           | 217          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 535          |\\n\",\n",
    "      \"|    ep_rew_mean          | 25.6         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 3408         |\\n\",\n",
    "      \"|    iterations           | 18           |\\n\",\n",
    "      \"|    time_elapsed         | 86           |\\n\",\n",
    "      \"|    total_timesteps      | 294912       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0036145751 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0105       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.22        |\\n\",\n",
    "      \"|    explained_variance   | 0.68778306   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 129          |\\n\",\n",
    "      \"|    n_updates            | 68           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000293    |\\n\",\n",
    "      \"|    value_loss           | 217          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 616          |\\n\",\n",
    "      \"|    ep_rew_mean          | 30.6         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 3284         |\\n\",\n",
    "      \"|    iterations           | 19           |\\n\",\n",
    "      \"|    time_elapsed         | 94           |\\n\",\n",
    "      \"|    total_timesteps      | 311296       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0076236688 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0418       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.19        |\\n\",\n",
    "      \"|    explained_variance   | 0.5854833    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 160          |\\n\",\n",
    "      \"|    n_updates            | 72           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00141     |\\n\",\n",
    "      \"|    value_loss           | 349          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 683          |\\n\",\n",
    "      \"|    ep_rew_mean          | 40.5         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 3153         |\\n\",\n",
    "      \"|    iterations           | 20           |\\n\",\n",
    "      \"|    time_elapsed         | 103          |\\n\",\n",
    "      \"|    total_timesteps      | 327680       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0038341046 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0266       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.15        |\\n\",\n",
    "      \"|    explained_variance   | 0.74977416   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 51.5         |\\n\",\n",
    "      \"|    n_updates            | 76           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00231     |\\n\",\n",
    "      \"|    value_loss           | 198          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 757         |\\n\",\n",
    "      \"|    ep_rew_mean          | 47.7        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 3049        |\\n\",\n",
    "      \"|    iterations           | 21          |\\n\",\n",
    "      \"|    time_elapsed         | 112         |\\n\",\n",
    "      \"|    total_timesteps      | 344064      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003637864 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0296      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.15       |\\n\",\n",
    "      \"|    explained_variance   | 0.8228218   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 52.4        |\\n\",\n",
    "      \"|    n_updates            | 80          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.002      |\\n\",\n",
    "      \"|    value_loss           | 126         |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 788          |\\n\",\n",
    "      \"|    ep_rew_mean          | 55.5         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2967         |\\n\",\n",
    "      \"|    iterations           | 22           |\\n\",\n",
    "      \"|    time_elapsed         | 121          |\\n\",\n",
    "      \"|    total_timesteps      | 360448       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0055814185 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0404       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.14        |\\n\",\n",
    "      \"|    explained_variance   | 0.8678865    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 40.6         |\\n\",\n",
    "      \"|    n_updates            | 84           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00206     |\\n\",\n",
    "      \"|    value_loss           | 93           |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 814          |\\n\",\n",
    "      \"|    ep_rew_mean          | 70.1         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2912         |\\n\",\n",
    "      \"|    iterations           | 23           |\\n\",\n",
    "      \"|    time_elapsed         | 129          |\\n\",\n",
    "      \"|    total_timesteps      | 376832       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0070961807 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0338       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.13        |\\n\",\n",
    "      \"|    explained_variance   | 0.91244507   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 17.4         |\\n\",\n",
    "      \"|    n_updates            | 88           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000656    |\\n\",\n",
    "      \"|    value_loss           | 74.3         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 870          |\\n\",\n",
    "      \"|    ep_rew_mean          | 83.3         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2841         |\\n\",\n",
    "      \"|    iterations           | 24           |\\n\",\n",
    "      \"|    time_elapsed         | 138          |\\n\",\n",
    "      \"|    total_timesteps      | 393216       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0062850155 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0285       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.12        |\\n\",\n",
    "      \"|    explained_variance   | 0.89901024   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 35.9         |\\n\",\n",
    "      \"|    n_updates            | 92           |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00201     |\\n\",\n",
    "      \"|    value_loss           | 92           |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"----------------------------------------\\n\",\n",
    "      \"| rollout/                |            |\\n\",\n",
    "      \"|    ep_len_mean          | 904        |\\n\",\n",
    "      \"|    ep_rew_mean          | 90.8       |\\n\",\n",
    "      \"| time/                   |            |\\n\",\n",
    "      \"|    fps                  | 2796       |\\n\",\n",
    "      \"|    iterations           | 25         |\\n\",\n",
    "      \"|    time_elapsed         | 146        |\\n\",\n",
    "      \"|    total_timesteps      | 409600     |\\n\",\n",
    "      \"| train/                  |            |\\n\",\n",
    "      \"|    approx_kl            | 0.00609289 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0365     |\\n\",\n",
    "      \"|    clip_range           | 0.2        |\\n\",\n",
    "      \"|    entropy_loss         | -1.14      |\\n\",\n",
    "      \"|    explained_variance   | 0.94261056 |\\n\",\n",
    "      \"|    learning_rate        | 0.0003     |\\n\",\n",
    "      \"|    loss                 | 55.9       |\\n\",\n",
    "      \"|    n_updates            | 96         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00127   |\\n\",\n",
    "      \"|    value_loss           | 51.6       |\\n\",\n",
    "      \"----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 892          |\\n\",\n",
    "      \"|    ep_rew_mean          | 91.1         |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2755         |\\n\",\n",
    "      \"|    iterations           | 26           |\\n\",\n",
    "      \"|    time_elapsed         | 154          |\\n\",\n",
    "      \"|    total_timesteps      | 425984       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0078340005 |\\n\",\n",
    "      \"|    clip_fraction        | 0.059        |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.08        |\\n\",\n",
    "      \"|    explained_variance   | 0.90933496   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 20.1         |\\n\",\n",
    "      \"|    n_updates            | 100          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00145     |\\n\",\n",
    "      \"|    value_loss           | 70.5         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 890         |\\n\",\n",
    "      \"|    ep_rew_mean          | 95.2        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2715        |\\n\",\n",
    "      \"|    iterations           | 27          |\\n\",\n",
    "      \"|    time_elapsed         | 162         |\\n\",\n",
    "      \"|    total_timesteps      | 442368      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.007617544 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0416      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.04       |\\n\",\n",
    "      \"|    explained_variance   | 0.88202953  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 31.2        |\\n\",\n",
    "      \"|    n_updates            | 104         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.0011     |\\n\",\n",
    "      \"|    value_loss           | 106         |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 890         |\\n\",\n",
    "      \"|    ep_rew_mean          | 94.5        |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2682        |\\n\",\n",
    "      \"|    iterations           | 28          |\\n\",\n",
    "      \"|    time_elapsed         | 171         |\\n\",\n",
    "      \"|    total_timesteps      | 458752      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.006022135 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0309      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.06       |\\n\",\n",
    "      \"|    explained_variance   | 0.94605327  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 32.2        |\\n\",\n",
    "      \"|    n_updates            | 108         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00159    |\\n\",\n",
    "      \"|    value_loss           | 51.7        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 891          |\\n\",\n",
    "      \"|    ep_rew_mean          | 101          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2651         |\\n\",\n",
    "      \"|    iterations           | 29           |\\n\",\n",
    "      \"|    time_elapsed         | 179          |\\n\",\n",
    "      \"|    total_timesteps      | 475136       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0037718113 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0194       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.04        |\\n\",\n",
    "      \"|    explained_variance   | 0.924904     |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 9.43         |\\n\",\n",
    "      \"|    n_updates            | 112          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000762    |\\n\",\n",
    "      \"|    value_loss           | 75           |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 888         |\\n\",\n",
    "      \"|    ep_rew_mean          | 104         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2633        |\\n\",\n",
    "      \"|    iterations           | 30          |\\n\",\n",
    "      \"|    time_elapsed         | 186         |\\n\",\n",
    "      \"|    total_timesteps      | 491520      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003552631 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0199      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.06       |\\n\",\n",
    "      \"|    explained_variance   | 0.9344727   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 5.86        |\\n\",\n",
    "      \"|    n_updates            | 116         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000169   |\\n\",\n",
    "      \"|    value_loss           | 69.6        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 887          |\\n\",\n",
    "      \"|    ep_rew_mean          | 109          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2607         |\\n\",\n",
    "      \"|    iterations           | 31           |\\n\",\n",
    "      \"|    time_elapsed         | 194          |\\n\",\n",
    "      \"|    total_timesteps      | 507904       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0036637858 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0287       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.05        |\\n\",\n",
    "      \"|    explained_variance   | 0.9690372    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 8.09         |\\n\",\n",
    "      \"|    n_updates            | 120          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00118     |\\n\",\n",
    "      \"|    value_loss           | 37.9         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 925         |\\n\",\n",
    "      \"|    ep_rew_mean          | 117         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2577        |\\n\",\n",
    "      \"|    iterations           | 32          |\\n\",\n",
    "      \"|    time_elapsed         | 203         |\\n\",\n",
    "      \"|    total_timesteps      | 524288      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003178807 |\\n\",\n",
    "      \"|    clip_fraction        | 0.026       |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -1.02       |\\n\",\n",
    "      \"|    explained_variance   | 0.96116966  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 17.5        |\\n\",\n",
    "      \"|    n_updates            | 124         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00054    |\\n\",\n",
    "      \"|    value_loss           | 46.7        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 934          |\\n\",\n",
    "      \"|    ep_rew_mean          | 120          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2546         |\\n\",\n",
    "      \"|    iterations           | 33           |\\n\",\n",
    "      \"|    time_elapsed         | 212          |\\n\",\n",
    "      \"|    total_timesteps      | 540672       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0041446737 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0343       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1.02        |\\n\",\n",
    "      \"|    explained_variance   | 0.9829329    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 3.76         |\\n\",\n",
    "      \"|    n_updates            | 128          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000824    |\\n\",\n",
    "      \"|    value_loss           | 15           |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 938          |\\n\",\n",
    "      \"|    ep_rew_mean          | 122          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2531         |\\n\",\n",
    "      \"|    iterations           | 34           |\\n\",\n",
    "      \"|    time_elapsed         | 220          |\\n\",\n",
    "      \"|    total_timesteps      | 557056       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0046944874 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0409       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -1           |\\n\",\n",
    "      \"|    explained_variance   | 0.9761899    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 4.05         |\\n\",\n",
    "      \"|    n_updates            | 132          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00116     |\\n\",\n",
    "      \"|    value_loss           | 27.2         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 948         |\\n\",\n",
    "      \"|    ep_rew_mean          | 123         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2516        |\\n\",\n",
    "      \"|    iterations           | 35          |\\n\",\n",
    "      \"|    time_elapsed         | 227         |\\n\",\n",
    "      \"|    total_timesteps      | 573440      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.002758705 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0202      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.971      |\\n\",\n",
    "      \"|    explained_variance   | 0.97679925  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 10.9        |\\n\",\n",
    "      \"|    n_updates            | 136         |\\n\",\n",
    "      \"|    policy_gradient_loss | -8.64e-05   |\\n\",\n",
    "      \"|    value_loss           | 28          |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 962          |\\n\",\n",
    "      \"|    ep_rew_mean          | 126          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2507         |\\n\",\n",
    "      \"|    iterations           | 36           |\\n\",\n",
    "      \"|    time_elapsed         | 235          |\\n\",\n",
    "      \"|    total_timesteps      | 589824       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0034285267 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0224       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.917       |\\n\",\n",
    "      \"|    explained_variance   | 0.96932834   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 6.63         |\\n\",\n",
    "      \"|    n_updates            | 140          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00054     |\\n\",\n",
    "      \"|    value_loss           | 35.7         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 969          |\\n\",\n",
    "      \"|    ep_rew_mean          | 130          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2494         |\\n\",\n",
    "      \"|    iterations           | 37           |\\n\",\n",
    "      \"|    time_elapsed         | 243          |\\n\",\n",
    "      \"|    total_timesteps      | 606208       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0039816983 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0486       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.939       |\\n\",\n",
    "      \"|    explained_variance   | 0.9924779    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 1.51         |\\n\",\n",
    "      \"|    n_updates            | 144          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000103    |\\n\",\n",
    "      \"|    value_loss           | 7.58         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 969         |\\n\",\n",
    "      \"|    ep_rew_mean          | 134         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2478        |\\n\",\n",
    "      \"|    iterations           | 38          |\\n\",\n",
    "      \"|    time_elapsed         | 251         |\\n\",\n",
    "      \"|    total_timesteps      | 622592      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003750179 |\\n\",\n",
    "      \"|    clip_fraction        | 0.049       |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.953      |\\n\",\n",
    "      \"|    explained_variance   | 0.98157746  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 3.08        |\\n\",\n",
    "      \"|    n_updates            | 148         |\\n\",\n",
    "      \"|    policy_gradient_loss | 4.26e-05    |\\n\",\n",
    "      \"|    value_loss           | 21.3        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 975         |\\n\",\n",
    "      \"|    ep_rew_mean          | 136         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2462        |\\n\",\n",
    "      \"|    iterations           | 39          |\\n\",\n",
    "      \"|    time_elapsed         | 259         |\\n\",\n",
    "      \"|    total_timesteps      | 638976      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.004494711 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0446      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.976      |\\n\",\n",
    "      \"|    explained_variance   | 0.9911582   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 1.89        |\\n\",\n",
    "      \"|    n_updates            | 152         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000175   |\\n\",\n",
    "      \"|    value_loss           | 9.78        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 982         |\\n\",\n",
    "      \"|    ep_rew_mean          | 138         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2447        |\\n\",\n",
    "      \"|    iterations           | 40          |\\n\",\n",
    "      \"|    time_elapsed         | 267         |\\n\",\n",
    "      \"|    total_timesteps      | 655360      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.004116741 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0531      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.952      |\\n\",\n",
    "      \"|    explained_variance   | 0.9945944   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 1.93        |\\n\",\n",
    "      \"|    n_updates            | 156         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00039    |\\n\",\n",
    "      \"|    value_loss           | 6.03        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 987         |\\n\",\n",
    "      \"|    ep_rew_mean          | 141         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2427        |\\n\",\n",
    "      \"|    iterations           | 41          |\\n\",\n",
    "      \"|    time_elapsed         | 276         |\\n\",\n",
    "      \"|    total_timesteps      | 671744      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.002762522 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0355      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.928      |\\n\",\n",
    "      \"|    explained_variance   | 0.99510556  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 1.8         |\\n\",\n",
    "      \"|    n_updates            | 160         |\\n\",\n",
    "      \"|    policy_gradient_loss | 0.000662    |\\n\",\n",
    "      \"|    value_loss           | 5.45        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 987          |\\n\",\n",
    "      \"|    ep_rew_mean          | 142          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2411         |\\n\",\n",
    "      \"|    iterations           | 42           |\\n\",\n",
    "      \"|    time_elapsed         | 285          |\\n\",\n",
    "      \"|    total_timesteps      | 688128       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0043645618 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0392       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.925       |\\n\",\n",
    "      \"|    explained_variance   | 0.9816638    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 11.1         |\\n\",\n",
    "      \"|    n_updates            | 164          |\\n\",\n",
    "      \"|    policy_gradient_loss | -6.29e-05    |\\n\",\n",
    "      \"|    value_loss           | 27.2         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 979          |\\n\",\n",
    "      \"|    ep_rew_mean          | 141          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2399         |\\n\",\n",
    "      \"|    iterations           | 43           |\\n\",\n",
    "      \"|    time_elapsed         | 293          |\\n\",\n",
    "      \"|    total_timesteps      | 704512       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0046475856 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0597       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.916       |\\n\",\n",
    "      \"|    explained_variance   | 0.99091864   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 2.14         |\\n\",\n",
    "      \"|    n_updates            | 168          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00182     |\\n\",\n",
    "      \"|    value_loss           | 8.27         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 979         |\\n\",\n",
    "      \"|    ep_rew_mean          | 140         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2395        |\\n\",\n",
    "      \"|    iterations           | 44          |\\n\",\n",
    "      \"|    time_elapsed         | 300         |\\n\",\n",
    "      \"|    total_timesteps      | 720896      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003335826 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0253      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.89       |\\n\",\n",
    "      \"|    explained_variance   | 0.9648079   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 65.2        |\\n\",\n",
    "      \"|    n_updates            | 172         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000603   |\\n\",\n",
    "      \"|    value_loss           | 51.8        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 965          |\\n\",\n",
    "      \"|    ep_rew_mean          | 143          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2386         |\\n\",\n",
    "      \"|    iterations           | 45           |\\n\",\n",
    "      \"|    time_elapsed         | 308          |\\n\",\n",
    "      \"|    total_timesteps      | 737280       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0033067747 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0451       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.854       |\\n\",\n",
    "      \"|    explained_variance   | 0.9966497    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 0.885        |\\n\",\n",
    "      \"|    n_updates            | 176          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000851    |\\n\",\n",
    "      \"|    value_loss           | 3.35         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 941          |\\n\",\n",
    "      \"|    ep_rew_mean          | 143          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2385         |\\n\",\n",
    "      \"|    iterations           | 46           |\\n\",\n",
    "      \"|    time_elapsed         | 315          |\\n\",\n",
    "      \"|    total_timesteps      | 753664       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0037576507 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0254       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.802       |\\n\",\n",
    "      \"|    explained_variance   | 0.9635501    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 4.65         |\\n\",\n",
    "      \"|    n_updates            | 180          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00136     |\\n\",\n",
    "      \"|    value_loss           | 59.6         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 947          |\\n\",\n",
    "      \"|    ep_rew_mean          | 143          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2378         |\\n\",\n",
    "      \"|    iterations           | 47           |\\n\",\n",
    "      \"|    time_elapsed         | 323          |\\n\",\n",
    "      \"|    total_timesteps      | 770048       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0025602723 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0265       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.815       |\\n\",\n",
    "      \"|    explained_variance   | 0.95954835   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 33           |\\n\",\n",
    "      \"|    n_updates            | 184          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000168    |\\n\",\n",
    "      \"|    value_loss           | 63.3         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 946          |\\n\",\n",
    "      \"|    ep_rew_mean          | 146          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2373         |\\n\",\n",
    "      \"|    iterations           | 48           |\\n\",\n",
    "      \"|    time_elapsed         | 331          |\\n\",\n",
    "      \"|    total_timesteps      | 786432       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0035295147 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0505       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.828       |\\n\",\n",
    "      \"|    explained_variance   | 0.99585426   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 0.864        |\\n\",\n",
    "      \"|    n_updates            | 188          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000214    |\\n\",\n",
    "      \"|    value_loss           | 3.58         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 948          |\\n\",\n",
    "      \"|    ep_rew_mean          | 148          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2370         |\\n\",\n",
    "      \"|    iterations           | 49           |\\n\",\n",
    "      \"|    time_elapsed         | 338          |\\n\",\n",
    "      \"|    total_timesteps      | 802816       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0031248182 |\\n\",\n",
    "      \"|    clip_fraction        | 0.037        |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.805       |\\n\",\n",
    "      \"|    explained_variance   | 0.98309124   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 65.1         |\\n\",\n",
    "      \"|    n_updates            | 192          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000506    |\\n\",\n",
    "      \"|    value_loss           | 24.2         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 948         |\\n\",\n",
    "      \"|    ep_rew_mean          | 149         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2364        |\\n\",\n",
    "      \"|    iterations           | 50          |\\n\",\n",
    "      \"|    time_elapsed         | 346         |\\n\",\n",
    "      \"|    total_timesteps      | 819200      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.004847469 |\\n\",\n",
    "      \"|    clip_fraction        | 0.043       |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.815      |\\n\",\n",
    "      \"|    explained_variance   | 0.9711339   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 2.51        |\\n\",\n",
    "      \"|    n_updates            | 196         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000545   |\\n\",\n",
    "      \"|    value_loss           | 40.2        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 960         |\\n\",\n",
    "      \"|    ep_rew_mean          | 150         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2364        |\\n\",\n",
    "      \"|    iterations           | 51          |\\n\",\n",
    "      \"|    time_elapsed         | 353         |\\n\",\n",
    "      \"|    total_timesteps      | 835584      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003944801 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0439      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.809      |\\n\",\n",
    "      \"|    explained_variance   | 0.99726003  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 1.44        |\\n\",\n",
    "      \"|    n_updates            | 200         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000653   |\\n\",\n",
    "      \"|    value_loss           | 2.16        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 958         |\\n\",\n",
    "      \"|    ep_rew_mean          | 151         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2362        |\\n\",\n",
    "      \"|    iterations           | 52          |\\n\",\n",
    "      \"|    time_elapsed         | 360         |\\n\",\n",
    "      \"|    total_timesteps      | 851968      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003368446 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0401      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.797      |\\n\",\n",
    "      \"|    explained_variance   | 0.98608184  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 1.84        |\\n\",\n",
    "      \"|    n_updates            | 204         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000588   |\\n\",\n",
    "      \"|    value_loss           | 21.9        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 955         |\\n\",\n",
    "      \"|    ep_rew_mean          | 155         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2359        |\\n\",\n",
    "      \"|    iterations           | 53          |\\n\",\n",
    "      \"|    time_elapsed         | 368         |\\n\",\n",
    "      \"|    total_timesteps      | 868352      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.004819712 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0295      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.799      |\\n\",\n",
    "      \"|    explained_variance   | 0.94915175  |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 4.12        |\\n\",\n",
    "      \"|    n_updates            | 208         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00135    |\\n\",\n",
    "      \"|    value_loss           | 89.3        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 953          |\\n\",\n",
    "      \"|    ep_rew_mean          | 156          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2356         |\\n\",\n",
    "      \"|    iterations           | 54           |\\n\",\n",
    "      \"|    time_elapsed         | 375          |\\n\",\n",
    "      \"|    total_timesteps      | 884736       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0036492767 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0467       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.779       |\\n\",\n",
    "      \"|    explained_variance   | 0.98521394   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 9.73         |\\n\",\n",
    "      \"|    n_updates            | 212          |\\n\",\n",
    "      \"|    policy_gradient_loss | 0.000508     |\\n\",\n",
    "      \"|    value_loss           | 21.3         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 948          |\\n\",\n",
    "      \"|    ep_rew_mean          | 160          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2352         |\\n\",\n",
    "      \"|    iterations           | 55           |\\n\",\n",
    "      \"|    time_elapsed         | 383          |\\n\",\n",
    "      \"|    total_timesteps      | 901120       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0047472557 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0401       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.762       |\\n\",\n",
    "      \"|    explained_variance   | 0.97441256   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 100          |\\n\",\n",
    "      \"|    n_updates            | 216          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00017     |\\n\",\n",
    "      \"|    value_loss           | 37.6         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 941         |\\n\",\n",
    "      \"|    ep_rew_mean          | 163         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2349        |\\n\",\n",
    "      \"|    iterations           | 56          |\\n\",\n",
    "      \"|    time_elapsed         | 390         |\\n\",\n",
    "      \"|    total_timesteps      | 917504      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.004006267 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0473      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.728      |\\n\",\n",
    "      \"|    explained_variance   | 0.9672255   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 90.6        |\\n\",\n",
    "      \"|    n_updates            | 220         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000581   |\\n\",\n",
    "      \"|    value_loss           | 54.1        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 927          |\\n\",\n",
    "      \"|    ep_rew_mean          | 163          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2347         |\\n\",\n",
    "      \"|    iterations           | 57           |\\n\",\n",
    "      \"|    time_elapsed         | 397          |\\n\",\n",
    "      \"|    total_timesteps      | 933888       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0046812575 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0634       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.753       |\\n\",\n",
    "      \"|    explained_variance   | 0.97306013   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 49.9         |\\n\",\n",
    "      \"|    n_updates            | 224          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000963    |\\n\",\n",
    "      \"|    value_loss           | 34.6         |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"| rollout/                |             |\\n\",\n",
    "      \"|    ep_len_mean          | 913         |\\n\",\n",
    "      \"|    ep_rew_mean          | 174         |\\n\",\n",
    "      \"| time/                   |             |\\n\",\n",
    "      \"|    fps                  | 2346        |\\n\",\n",
    "      \"|    iterations           | 58          |\\n\",\n",
    "      \"|    time_elapsed         | 405         |\\n\",\n",
    "      \"|    total_timesteps      | 950272      |\\n\",\n",
    "      \"| train/                  |             |\\n\",\n",
    "      \"|    approx_kl            | 0.003419607 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0563      |\\n\",\n",
    "      \"|    clip_range           | 0.2         |\\n\",\n",
    "      \"|    entropy_loss         | -0.73       |\\n\",\n",
    "      \"|    explained_variance   | 0.9524795   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003      |\\n\",\n",
    "      \"|    loss                 | 30.1        |\\n\",\n",
    "      \"|    n_updates            | 228         |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00169    |\\n\",\n",
    "      \"|    value_loss           | 79.4        |\\n\",\n",
    "      \"-----------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 882          |\\n\",\n",
    "      \"|    ep_rew_mean          | 176          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2346         |\\n\",\n",
    "      \"|    iterations           | 59           |\\n\",\n",
    "      \"|    time_elapsed         | 411          |\\n\",\n",
    "      \"|    total_timesteps      | 966656       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0048039174 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0634       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.698       |\\n\",\n",
    "      \"|    explained_variance   | 0.9130458    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 173          |\\n\",\n",
    "      \"|    n_updates            | 232          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00211     |\\n\",\n",
    "      \"|    value_loss           | 161          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 812          |\\n\",\n",
    "      \"|    ep_rew_mean          | 191          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2350         |\\n\",\n",
    "      \"|    iterations           | 60           |\\n\",\n",
    "      \"|    time_elapsed         | 418          |\\n\",\n",
    "      \"|    total_timesteps      | 983040       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0034556773 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0413       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.677       |\\n\",\n",
    "      \"|    explained_variance   | 0.9273998    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 86.3         |\\n\",\n",
    "      \"|    n_updates            | 236          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.000829    |\\n\",\n",
    "      \"|    value_loss           | 127          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 674          |\\n\",\n",
    "      \"|    ep_rew_mean          | 212          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2360         |\\n\",\n",
    "      \"|    iterations           | 61           |\\n\",\n",
    "      \"|    time_elapsed         | 423          |\\n\",\n",
    "      \"|    total_timesteps      | 999424       |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0056466972 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0711       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.688       |\\n\",\n",
    "      \"|    explained_variance   | 0.8850324    |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 81.9         |\\n\",\n",
    "      \"|    n_updates            | 240          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00205     |\\n\",\n",
    "      \"|    value_loss           | 219          |\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"------------------------------------------\\n\",\n",
    "      \"| rollout/                |              |\\n\",\n",
    "      \"|    ep_len_mean          | 514          |\\n\",\n",
    "      \"|    ep_rew_mean          | 234          |\\n\",\n",
    "      \"| time/                   |              |\\n\",\n",
    "      \"|    fps                  | 2369         |\\n\",\n",
    "      \"|    iterations           | 62           |\\n\",\n",
    "      \"|    time_elapsed         | 428          |\\n\",\n",
    "      \"|    total_timesteps      | 1015808      |\\n\",\n",
    "      \"| train/                  |              |\\n\",\n",
    "      \"|    approx_kl            | 0.0065626795 |\\n\",\n",
    "      \"|    clip_fraction        | 0.0937       |\\n\",\n",
    "      \"|    clip_range           | 0.2          |\\n\",\n",
    "      \"|    entropy_loss         | -0.737       |\\n\",\n",
    "      \"|    explained_variance   | 0.83415127   |\\n\",\n",
    "      \"|    learning_rate        | 0.0003       |\\n\",\n",
    "      \"|    loss                 | 129          |\\n\",\n",
    "      \"|    n_updates            | 244          |\\n\",\n",
    "      \"|    policy_gradient_loss | -0.00305     |\\n\",\n",
    "      \"|    value_loss           | 304          |\\n\",\n",
    "      \"------------------------------------------\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# SOLUTION\\n\",\n",
    "    \"# Train it for 1,000,000 timesteps\\n\",\n",
    "    \"model.learn(total_timesteps=1000000)\\n\",\n",
    "    \"# Save the model\\n\",\n",
    "    \"model_name = \\\"ppo-LunarLander-v2\\\"\\n\",\n",
    "    \"model.save(model_name)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BY_HuedOoISR\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Evaluate the agent üìà\\n\",\n",
    "    \"- Remember to wrap the environment in a [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html).\\n\",\n",
    "    \"- Now that our Lunar Lander agent is trained üöÄ, we need to **check its performance**.\\n\",\n",
    "    \"- Stable-Baselines3 provides a method to do that: `evaluate_policy`.\\n\",\n",
    "    \"- To fill that part you need to [check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\\n\",\n",
    "    \"- In the next step,  we'll see **how to automatically evaluate and share your agent to compete in a leaderboard, but for now let's do it ourselves**\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"üí° When you evaluate your agent, you should not use your training environment but create an evaluation environment.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"yRpno0glsADy\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# TODO: Evaluate the agent\\n\",\n",
    "    \"# Create a new environment for evaluation\\n\",\n",
    "    \"# eval_env = Monitor(gym.make(\\\"LunarLander-v2\\\", render_mode='rgb_array'))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate the model with 10 evaluation episodes and deterministic=True\\n\",\n",
    "    \"# mean_reward, std_reward = evaluate_policy(model, env, deterministic=True, n_eval_episodes=10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print the results\\n\",\n",
    "    \"# print(f\\\"mean_reward={mean_reward:.2f} +/- {std_reward}\\\")\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BqPKw3jt_pG5\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"#### Solution\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 10,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"zpz8kHlt_a_m\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"mean_reward=248.81 +/- 19.30419004915218\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"#@title\\n\",\n",
    "    \"eval_env = Monitor(gym.make(\\\"LunarLander-v2\\\", render_mode='rgb_array'))\\n\",\n",
    "    \"mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\\n\",\n",
    "    \"print(f\\\"mean_reward={mean_reward:.2f} +/- {std_reward}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"reBhoODwcXfr\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"- In my case, I got a mean reward of `200.20 +/- 20.80` after training for 1 million steps, which means that our lunar lander agent is ready to land on the moon üåõü•≥.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"IK_kR78NoNb2\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Publish our trained model on the Hub üî•\\n\",\n",
    "    \"Now that we saw we got good results after the training, we can publish our trained model on the hub ü§ó with one line of code.\\n\",\n",
    "    \"\\n\",\n",
    "    \"üìö The libraries documentation üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here's an example of a Model Card (with Space Invaders):\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Gs-Ew7e1gXN3\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"By using `package_to_hub` **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\\n\",\n",
    "    \"\\n\",\n",
    "    \"This way:\\n\",\n",
    "    \"- You can **showcase our work** üî•\\n\",\n",
    "    \"- You can **visualize your agent playing** üëÄ\\n\",\n",
    "    \"- You can **share with the community an agent that others can use** üíæ\\n\",\n",
    "    \"- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"JquRrWytA6eo\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"To be able to share your model with the community there are three more steps to follow:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1Ô∏è‚É£ (If it's not already done) create an account on Hugging Face ‚û° https://huggingface.co/join\\n\",\n",
    "    \"\\n\",\n",
    "    \"2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\\n\",\n",
    "    \"- Create a new token (https://huggingface.co/settings/tokens) **with write role**\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\\\" alt=\\\"Create HF Token\\\">\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Copy the token\\n\",\n",
    "    \"- Run the cell below and paste the token\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 11,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"GZiFBBlzxzxY\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"03dbd1e7b52e45899357426c46e1edb1\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"VBox(children=(HTML(value='<center> <img\\\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"notebook_login()\\n\",\n",
    "    \"!git config --global credential.helper store\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"_tsf2uv0g_4p\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"FGNh9VsZok0i\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()` function\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Ay24l6bqFF18\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Let's fill the `package_to_hub` function:\\n\",\n",
    "    \"- `model`: our trained model.\\n\",\n",
    "    \"- `model_name`: the name of the trained model that we defined in `model_save`\\n\",\n",
    "    \"- `model_architecture`: the model architecture we used, in our case PPO\\n\",\n",
    "    \"- `env_id`: the name of the environment, in our case `LunarLander-v2`\\n\",\n",
    "    \"- `eval_env`: the evaluation environment defined in eval_env\\n\",\n",
    "    \"- `repo_id`: the name of the Hugging Face Hub Repository that will be created/updated `(repo_id = {username}/{repo_name})`\\n\",\n",
    "    \"\\n\",\n",
    "    \"üí° **A good name is {username}/{model_architecture}-{env_id}**\\n\",\n",
    "    \"\\n\",\n",
    "    \"- `commit_message`: message of the commit\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"JPG7ofdGIHN8\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001b[38;5;4m‚Ñπ This function will save, evaluate, generate a video of your agent,\\n\",\n",
    "      \"create a model card and push everything to the hub. It might take up to 1min.\\n\",\n",
    "      \"This is a work in progress: if you encounter a bug, please open an issue.\\u001b[0m\\n\",\n",
    "      \"Saving video to /tmp/tmppuy2bp69/-step-0-to-step-1000.mp4\\n\",\n",
    "      \"Moviepy - Building video /tmp/tmppuy2bp69/-step-0-to-step-1000.mp4.\\n\",\n",
    "      \"Moviepy - Writing video /tmp/tmppuy2bp69/-step-0-to-step-1000.mp4\\n\",\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"ffmpeg version 7.1.1 Copyright (c) 2000-2025 the FFmpeg developers                                                                                    \\n\",\n",
    "      \"  built with gcc 13.3.0 (conda-forge gcc 13.3.0-2)\\n\",\n",
    "      \"  configuration: --prefix=/home/ekami/workspace/deep-rl-class/notebooks/unit1/.pixi/envs/default --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1743376039040/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1743376039040/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1743376039040/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1743376039040/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --enable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --disable-gnutls --enable-libvpx --enable-libass --enable-pthreads --enable-alsa --enable-libpulse --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libmp3lame --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libvorbis --enable-libopus --enable-librsvg --enable-ffplay --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1743376039040/_build_env/bin/pkg-config\\n\",\n",
    "      \"  libavutil      59. 39.100 / 59. 39.100\\n\",\n",
    "      \"  libavcodec     61. 19.101 / 61. 19.101\\n\",\n",
    "      \"  libavformat    61.  7.100 / 61.  7.100\\n\",\n",
    "      \"  libavdevice    61.  3.100 / 61.  3.100\\n\",\n",
    "      \"  libavfilter    10.  4.100 / 10.  4.100\\n\",\n",
    "      \"  libswscale      8.  3.100 /  8.  3.100\\n\",\n",
    "      \"  libswresample   5.  3.100 /  5.  3.100\\n\",\n",
    "      \"  libpostproc    58.  3.100 / 58.  3.100\\n\",\n",
    "      \"Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/tmp/tmppuy2bp69/-step-0-to-step-1000.mp4':\\n\",\n",
    "      \"  Metadata:\\n\",\n",
    "      \"    major_brand     : isom\\n\",\n",
    "      \"    minor_version   : 512\\n\",\n",
    "      \"    compatible_brands: isomiso2avc1mp41\\n\",\n",
    "      \"    encoder         : Lavf61.7.100\\n\",\n",
    "      \"  Duration: 00:00:20.02, start: 0.000000, bitrate: 63 kb/s\\n\",\n",
    "      \"  Stream #0:0[0x1](und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(progressive), 600x400, 58 kb/s, 50 fps, 50 tbr, 12800 tbn (default)\\n\",\n",
    "      \"      Metadata:\\n\",\n",
    "      \"        handler_name    : VideoHandler\\n\",\n",
    "      \"        vendor_id       : [0][0][0][0]\\n\",\n",
    "      \"        encoder         : Lavc61.19.101 libx264\\n\",\n",
    "      \"Stream mapping:\\n\",\n",
    "      \"  Stream #0:0 -> #0:0 (h264 (native) -> h264 (libx264))\\n\",\n",
    "      \"Press [q] to stop, [?] for help\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] profile High, level 3.1, 4:2:0, 8-bit\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] 264 - core 164 r3095 baee400 - H.264/MPEG-4 AVC codec - Copyleft 2003-2022 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=12 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=25 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\\n\",\n",
    "      \"Output #0, mp4, to '/tmp/tmp2s_bm35y/replay.mp4':\\n\",\n",
    "      \"  Metadata:\\n\",\n",
    "      \"    major_brand     : isom\\n\",\n",
    "      \"    minor_version   : 512\\n\",\n",
    "      \"    compatible_brands: isomiso2avc1mp41\\n\",\n",
    "      \"    encoder         : Lavf61.7.100\\n\",\n",
    "      \"  Stream #0:0(und): Video: h264 (avc1 / 0x31637661), yuv420p(tv, progressive), 600x400, q=2-31, 50 fps, 12800 tbn (default)\\n\",\n",
    "      \"      Metadata:\\n\",\n",
    "      \"        handler_name    : VideoHandler\\n\",\n",
    "      \"        vendor_id       : [0][0][0][0]\\n\",\n",
    "      \"        encoder         : Lavc61.19.101 libx264\\n\",\n",
    "      \"      Side data:\\n\",\n",
    "      \"        cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Moviepy - Done !\\n\",\n",
    "      \"Moviepy - video ready /tmp/tmppuy2bp69/-step-0-to-step-1000.mp4\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"[out#0/mp4 @ 0x62480fba9e80] video:138KiB audio:0KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 9.064543%\\n\",\n",
    "      \"frame= 1001 fps=0.0 q=-1.0 Lsize=     150KiB time=00:00:19.98 bitrate=  61.6kbits/s speed=75.5x    \\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] frame I:5     Avg QP:13.30  size:  1910\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] frame P:260   Avg QP:22.29  size:   227\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] frame B:736   Avg QP:21.89  size:    97\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] consecutive B-frames:  0.9%  2.0%  3.6% 93.5%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] mb I  I16..4: 68.4% 25.2%  6.4%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] mb P  I16..4:  0.3%  0.4%  0.2%  P16..4:  1.9%  0.5%  0.2%  0.0%  0.0%    skip:96.6%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] mb B  I16..4:  0.1%  0.0%  0.0%  B16..8:  2.5%  0.2%  0.0%  direct: 0.1%  skip:97.0%  L0:56.1% L1:43.0% BI: 0.9%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] 8x8 transform intra:28.9% inter:15.9%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] coded y,uvDC,uvAC intra: 6.1% 8.7% 7.9% inter: 0.1% 0.2% 0.2%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] i16 v,h,dc,p: 68% 27%  5%  0%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33%  7% 59%  0%  0%  0%  0%  0%  1%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 13% 17% 58%  2%  2%  1%  3%  1%  3%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] i8c dc,h,v,p: 93%  5%  3%  0%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] Weighted P-Frames: Y:0.0% UV:0.0%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] ref P L0: 68.2%  2.2% 18.6% 11.1%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] ref B L0: 69.3% 26.4%  4.3%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] ref B L1: 94.2%  5.8%\\n\",\n",
    "      \"[libx264 @ 0x62480fba3800] kb/s:56.08\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001b[38;5;4m‚Ñπ Pushing repo Ekami/PPO-LunarLander-v2 to the Hugging Face Hub\\u001b[0m\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"39ac25f52155497bbfcfb2d66f690fac\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"policy.optimizer.pth:   0%|          | 0.00/88.4k [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"a009a7e24a1044eb9bb2bd7e9a9d770b\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"policy.pth:   0%|          | 0.00/43.8k [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"fb4d17f27c6844c4ac424a3019dbfb15\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"pytorch_variables.pth:   0%|          | 0.00/864 [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"703c7e06b66949d5b9e6444445d58326\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"ppo-LunarLander-v2.zip:   0%|          | 0.00/148k [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"8459cdfcbe394c9dbf0af40c325035a5\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"fa0eb42c7d8a4da78e4b82494df97b78\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"replay.mp4:   0%|          | 0.00/154k [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"'(MaxRetryError(\\\"HTTPSConnectionPool(host='hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com', port=443): Max retries exceeded with url: /repos/75/22/752202b510656eebd3bfef8d4f2d1281c6b44acf31abfaa40a71b85cda3d30dc/0f6a3611582182117ed4911cffb24764175e0967d55123f5deaba2b0077d8e4e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250424T115006Z&X-Amz-Expires=900&X-Amz-Signature=65937b9fabb39841e85fc1d4309a8dc370312074ebb203a8d72edfb36f192f69&X-Amz-SignedHeaders=host&x-amz-storage-class=INTELLIGENT_TIERING&x-id=PutObject (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7bafdc06e5d0>, 'Connection to hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com timed out. (connect timeout=None)'))\\\"), '(Request ID: c2233d24-07ca-49de-b8d8-a1b6060d310f)')' thrown while requesting PUT https://hf-hub-lfs-us-east-1.s3-accelerate.amazonaws.com/repos/75/22/752202b510656eebd3bfef8d4f2d1281c6b44acf31abfaa40a71b85cda3d30dc/0f6a3611582182117ed4911cffb24764175e0967d55123f5deaba2b0077d8e4e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIA2JU7TKAQLC2QXPN7%2F20250424%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250424T115006Z&X-Amz-Expires=900&X-Amz-Signature=65937b9fabb39841e85fc1d4309a8dc370312074ebb203a8d72edfb36f192f69&X-Amz-SignedHeaders=host&x-amz-storage-class=INTELLIGENT_TIERING&x-id=PutObject\\n\",\n",
    "      \"Retrying in 1s [Retry 1/5].\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001b[38;5;4m‚Ñπ Your model is pushed to the Hub. You can view your model here:\\n\",\n",
    "      \"https://huggingface.co/Ekami/PPO-LunarLander-v2/tree/main/\\u001b[0m\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"CommitInfo(commit_url='https://huggingface.co/Ekami/PPO-LunarLander-v2/commit/d733d7d29c1349bbeb5c073ea6df4286f08eb78e', commit_message='Initial commit', commit_description='', oid='d733d7d29c1349bbeb5c073ea6df4286f08eb78e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Ekami/PPO-LunarLander-v2', endpoint='https://huggingface.co', repo_type='model', repo_id='Ekami/PPO-LunarLander-v2'), pr_revision=None, pr_num=None)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 12,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"from stable_baselines3.common.vec_env import DummyVecEnv\\n\",\n",
    "    \"from stable_baselines3.common.env_util import make_vec_env\\n\",\n",
    "    \"\\n\",\n",
    "    \"from huggingface_sb3 import package_to_hub\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"model_architecture = \\\"PPO\\\"\\n\",\n",
    "    \"env_id = \\\"LunarLander-v2\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## TODO: Define a repo_id\\n\",\n",
    "    \"## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\\n\",\n",
    "    \"repo_id = f\\\"Ekami/{model_architecture}-{env_id}\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create the evaluation env and set the render_mode=\\\"rgb_array\\\"\\n\",\n",
    "    \"eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\\\"rgb_array\\\"))])\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## TODO: Define the commit message\\n\",\n",
    "    \"commit_message = \\\"Initial commit\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# method save, evaluate, generate a model card and record a replay video of your agent before pushing the repo to the hub\\n\",\n",
    "    \"package_to_hub(\\n\",\n",
    "    \"    model=model, # Our trained model\\n\",\n",
    "    \"    model_name=model_name, # The name of our trained model\\n\",\n",
    "    \"    model_architecture=model_architecture, # The model architecture we used: in our case PPO\\n\",\n",
    "    \"    env_id=env_id, # Name of the environment\\n\",\n",
    "    \"    eval_env=eval_env, # Evaluation Environment\\n\",\n",
    "    \"    repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\\n\",\n",
    "    \"    commit_message=commit_message\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Avf6gufJBGMw\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"#### Solution\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"I2E--IJu8JYq\"\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"\\n\",\n",
    "    \"from stable_baselines3 import PPO\\n\",\n",
    "    \"from stable_baselines3.common.vec_env import DummyVecEnv\\n\",\n",
    "    \"from stable_baselines3.common.env_util import make_vec_env\\n\",\n",
    "    \"\\n\",\n",
    "    \"from huggingface_sb3 import package_to_hub\\n\",\n",
    "    \"\\n\",\n",
    "    \"# PLACE the variables you've just defined two cells above\\n\",\n",
    "    \"# Define the name of the environment\\n\",\n",
    "    \"env_id = \\\"LunarLander-v2\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# TODO: Define the model architecture we used\\n\",\n",
    "    \"model_architecture = \\\"PPO\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Define a repo_id\\n\",\n",
    "    \"## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\\n\",\n",
    "    \"## CHANGE WITH YOUR REPO ID\\n\",\n",
    "    \"repo_id = \\\"ThomasSimonini/ppo-LunarLander-v2\\\" # Change with your repo id, you can't push with mine üòÑ\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Define the commit message\\n\",\n",
    "    \"commit_message = \\\"Upload PPO LunarLander-v2 trained agent\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create the evaluation env and set the render_mode=\\\"rgb_array\\\"\\n\",\n",
    "    \"eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\\\"rgb_array\\\")])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# PLACE the package_to_hub function you've just filled here\\n\",\n",
    "    \"package_to_hub(model=model, # Our trained model\\n\",\n",
    "    \"               model_name=model_name, # The name of our trained model\\n\",\n",
    "    \"               model_architecture=model_architecture, # The model architecture we used: in our case PPO\\n\",\n",
    "    \"               env_id=env_id, # Name of the environment\\n\",\n",
    "    \"               eval_env=eval_env, # Evaluation Environment\\n\",\n",
    "    \"               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\\n\",\n",
    "    \"               commit_message=commit_message)\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"T79AEAWEFIxz\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Congrats ü•≥ you've just trained and uploaded your first Deep Reinforcement Learning agent. The script above should have displayed a link to a model repository such as https://huggingface.co/osanseviero/test_sb3. When you go to this link, you can:\\n\",\n",
    "    \"* See a video preview of your agent at the right.\\n\",\n",
    "    \"* Click \\\"Files and versions\\\" to see all the files in the repository.\\n\",\n",
    "    \"* Click \\\"Use in stable-baselines3\\\" to get a code snippet that shows how to load the model.\\n\",\n",
    "    \"* A model card (`README.md` file) which gives a description of the model\\n\",\n",
    "    \"\\n\",\n",
    "    \"Under the hood, the Hub uses git-based repositories (don't worry if you don't know what git is), which means you can update the model with new versions as you experiment and improve your agent.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Compare the results of your LunarLander-v2 with your classmates using the leaderboard üèÜ üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"9nWnuQHRfFRa\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Load a saved LunarLander model from the Hub ü§ó\\n\",\n",
    "    \"Thanks to [ironbar](https://github.com/ironbar) for the contribution.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Loading a saved model from the Hub is really easy.\\n\",\n",
    "    \"\\n\",\n",
    "    \"You go to https://huggingface.co/models?library=stable-baselines3 to see the list of all the Stable-baselines3 saved models.\\n\",\n",
    "    \"1. You select one and copy its repo_id\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\\\" alt=\\\"Copy-id\\\"/>\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"hNPLJF2bfiUw\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"2. Then we just need to use load_from_hub with:\\n\",\n",
    "    \"- The repo_id\\n\",\n",
    "    \"- The filename: the saved model inside the repo and its extension (*.zip)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"bhb9-NtsinKB\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Because the model I download from the Hub was trained with Gym (the former version of Gymnasium) we need to install shimmy a API conversion tool that will help us to run the environment correctly.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Shimmy Documentation: https://github.com/Farama-Foundation/Shimmy\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 13,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"03WI-bkci1kH\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001b[1;31merror\\u001b[0m: \\u001b[1mexternally-managed-environment\\u001b[0m\\n\",\n",
    "      \"\\n\",\n",
    "      \"\\u001b[31m√ó\\u001b[0m This environment is externally managed\\n\",\n",
    "      \"\\u001b[31m‚ï∞‚îÄ>\\u001b[0m To install Python packages system-wide, try apt install\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m python3-xyz, where xyz is the package you are trying to\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m install.\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m \\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m If you wish to install a non-Debian-packaged Python package,\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m create a virtual environment using python3 -m venv path/to/venv.\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m sure you have python3-full installed.\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m \\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m If you wish to install a non-Debian packaged Python application,\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m it may be easiest to use pipx install xyz, which will manage a\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m virtual environment for you. Make sure you have pipx installed.\\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m \\n\",\n",
    "      \"\\u001b[31m   \\u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\\n\",\n",
    "      \"\\n\",\n",
    "      \"\\u001b[1;35mnote\\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\\n\",\n",
    "      \"\\u001b[1;36mhint\\u001b[0m: See PEP 668 for the detailed specification.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"!pip install shimmy\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 14,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"oj8PSGHJfwz3\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"49716bd8033e4ab5949080537e8f030b\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"ppo-LunarLander-v2.zip:   0%|          | 0.00/146k [00:00<?, ?B/s]\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"== CURRENT SYSTEM INFO ==\\n\",\n",
    "      \"- OS: Linux-6.8.0-58-generic-x86_64-with-glibc2.39 # 60-Ubuntu SMP PREEMPT_DYNAMIC Fri Mar 14 18:29:48 UTC 2025\\n\",\n",
    "      \"- Python: 3.11.12\\n\",\n",
    "      \"- Stable-Baselines3: 2.0.0\\n\",\n",
    "      \"- PyTorch: 2.6.0\\n\",\n",
    "      \"- GPU Enabled: True\\n\",\n",
    "      \"- Numpy: 2.2.5\\n\",\n",
    "      \"- Cloudpickle: 3.1.1\\n\",\n",
    "      \"- Gymnasium: 0.28.1\\n\",\n",
    "      \"\\n\",\n",
    "      \"== SAVED MODEL SYSTEM INFO ==\\n\",\n",
    "      \"OS: Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic #1 SMP Sun Apr 24 10:03:06 PDT 2022\\n\",\n",
    "      \"Python: 3.7.13\\n\",\n",
    "      \"Stable-Baselines3: 1.5.0\\n\",\n",
    "      \"PyTorch: 1.11.0+cu113\\n\",\n",
    "      \"GPU Enabled: True\\n\",\n",
    "      \"Numpy: 1.21.6\\n\",\n",
    "      \"Gym: 0.21.0\\n\",\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"ename\": \"ModuleNotFoundError\",\n",
    "     \"evalue\": \"No module named 'gym'\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001b[31m---------------------------------------------------------------------------\\u001b[39m\",\n",
    "      \"\\u001b[31mModuleNotFoundError\\u001b[39m                       Traceback (most recent call last)\",\n",
    "      \"\\u001b[36mCell\\u001b[39m\\u001b[36m \\u001b[39m\\u001b[32mIn[14]\\u001b[39m\\u001b[32m, line 17\\u001b[39m\\n\\u001b[32m     10\\u001b[39m custom_objects = {\\n\\u001b[32m     11\\u001b[39m             \\u001b[33m\\\"\\u001b[39m\\u001b[33mlearning_rate\\u001b[39m\\u001b[33m\\\"\\u001b[39m: \\u001b[32m0.0\\u001b[39m,\\n\\u001b[32m     12\\u001b[39m             \\u001b[33m\\\"\\u001b[39m\\u001b[33mlr_schedule\\u001b[39m\\u001b[33m\\\"\\u001b[39m: \\u001b[38;5;28;01mlambda\\u001b[39;00m _: \\u001b[32m0.0\\u001b[39m,\\n\\u001b[32m     13\\u001b[39m             \\u001b[33m\\\"\\u001b[39m\\u001b[33mclip_range\\u001b[39m\\u001b[33m\\\"\\u001b[39m: \\u001b[38;5;28;01mlambda\\u001b[39;00m _: \\u001b[32m0.0\\u001b[39m,\\n\\u001b[32m     14\\u001b[39m }\\n\\u001b[32m     16\\u001b[39m checkpoint = load_from_hub(repo_id, filename)\\n\\u001b[32m---> \\u001b[39m\\u001b[32m17\\u001b[39m model = \\u001b[43mPPO\\u001b[49m\\u001b[43m.\\u001b[49m\\u001b[43mload\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mcheckpoint\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mprint_system_info\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[38;5;28;43;01mTrue\\u001b[39;49;00m\\u001b[43m)\\u001b[49m\\n\",\n",
    "      \"\\u001b[36mFile \\u001b[39m\\u001b[32m~/workspace/deep-rl-class/notebooks/unit1/.pixi/envs/default/lib/python3.11/site-packages/stable_baselines3/common/base_class.py:679\\u001b[39m, in \\u001b[36mBaseAlgorithm.load\\u001b[39m\\u001b[34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\\u001b[39m\\n\\u001b[32m    676\\u001b[39m     \\u001b[38;5;28mprint\\u001b[39m(\\u001b[33m\\\"\\u001b[39m\\u001b[33m== CURRENT SYSTEM INFO ==\\u001b[39m\\u001b[33m\\\"\\u001b[39m)\\n\\u001b[32m    677\\u001b[39m     get_system_info()\\n\\u001b[32m--> \\u001b[39m\\u001b[32m679\\u001b[39m data, params, pytorch_variables = \\u001b[43mload_from_zip_file\\u001b[49m\\u001b[43m(\\u001b[49m\\n\\u001b[32m    680\\u001b[39m \\u001b[43m    \\u001b[49m\\u001b[43mpath\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[32m    681\\u001b[39m \\u001b[43m    \\u001b[49m\\u001b[43mdevice\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[43mdevice\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[32m    682\\u001b[39m \\u001b[43m    \\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[32m    683\\u001b[39m \\u001b[43m    \\u001b[49m\\u001b[43mprint_system_info\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[43mprint_system_info\\u001b[49m\\u001b[43m,\\u001b[49m\\n\\u001b[32m    684\\u001b[39m \\u001b[43m\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[32m    686\\u001b[39m \\u001b[38;5;28;01massert\\u001b[39;00m data \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m, \\u001b[33m\\\"\\u001b[39m\\u001b[33mNo data found in the saved file\\u001b[39m\\u001b[33m\\\"\\u001b[39m\\n\\u001b[32m    687\\u001b[39m \\u001b[38;5;28;01massert\\u001b[39;00m params \\u001b[38;5;129;01mis\\u001b[39;00m \\u001b[38;5;129;01mnot\\u001b[39;00m \\u001b[38;5;28;01mNone\\u001b[39;00m, \\u001b[33m\\\"\\u001b[39m\\u001b[33mNo params found in the saved file\\u001b[39m\\u001b[33m\\\"\\u001b[39m\\n\",\n",
    "      \"\\u001b[36mFile \\u001b[39m\\u001b[32m~/workspace/deep-rl-class/notebooks/unit1/.pixi/envs/default/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:421\\u001b[39m, in \\u001b[36mload_from_zip_file\\u001b[39m\\u001b[34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\\u001b[39m\\n\\u001b[32m    417\\u001b[39m \\u001b[38;5;28;01mif\\u001b[39;00m \\u001b[33m\\\"\\u001b[39m\\u001b[33mdata\\u001b[39m\\u001b[33m\\\"\\u001b[39m \\u001b[38;5;129;01min\\u001b[39;00m namelist \\u001b[38;5;129;01mand\\u001b[39;00m load_data:\\n\\u001b[32m    418\\u001b[39m     \\u001b[38;5;66;03m# Load class parameters that are stored\\u001b[39;00m\\n\\u001b[32m    419\\u001b[39m     \\u001b[38;5;66;03m# with either JSON or pickle (not PyTorch variables).\\u001b[39;00m\\n\\u001b[32m    420\\u001b[39m     json_data = archive.read(\\u001b[33m\\\"\\u001b[39m\\u001b[33mdata\\u001b[39m\\u001b[33m\\\"\\u001b[39m).decode()\\n\\u001b[32m--> \\u001b[39m\\u001b[32m421\\u001b[39m     data = \\u001b[43mjson_to_data\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mjson_data\\u001b[49m\\u001b[43m,\\u001b[49m\\u001b[43m \\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m=\\u001b[49m\\u001b[43mcustom_objects\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[32m    423\\u001b[39m \\u001b[38;5;66;03m# Check for all .pth files and load them using th.load.\\u001b[39;00m\\n\\u001b[32m    424\\u001b[39m \\u001b[38;5;66;03m# \\\"pytorch_variables.pth\\\" stores PyTorch variables, and any other .pth\\u001b[39;00m\\n\\u001b[32m    425\\u001b[39m \\u001b[38;5;66;03m# files store state_dicts of variables with custom names (e.g. policy, policy.optimizer)\\u001b[39;00m\\n\\u001b[32m    426\\u001b[39m pth_files = [file_name \\u001b[38;5;28;01mfor\\u001b[39;00m file_name \\u001b[38;5;129;01min\\u001b[39;00m namelist \\u001b[38;5;28;01mif\\u001b[39;00m os.path.splitext(file_name)[\\u001b[32m1\\u001b[39m] == \\u001b[33m\\\"\\u001b[39m\\u001b[33m.pth\\u001b[39m\\u001b[33m\\\"\\u001b[39m]\\n\",\n",
    "      \"\\u001b[36mFile \\u001b[39m\\u001b[32m~/workspace/deep-rl-class/notebooks/unit1/.pixi/envs/default/lib/python3.11/site-packages/stable_baselines3/common/save_util.py:164\\u001b[39m, in \\u001b[36mjson_to_data\\u001b[39m\\u001b[34m(json_string, custom_objects)\\u001b[39m\\n\\u001b[32m    162\\u001b[39m \\u001b[38;5;28;01mtry\\u001b[39;00m:\\n\\u001b[32m    163\\u001b[39m     base64_object = base64.b64decode(serialization.encode())\\n\\u001b[32m--> \\u001b[39m\\u001b[32m164\\u001b[39m     deserialized_object = \\u001b[43mcloudpickle\\u001b[49m\\u001b[43m.\\u001b[49m\\u001b[43mloads\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mbase64_object\\u001b[49m\\u001b[43m)\\u001b[49m\\n\\u001b[32m    165\\u001b[39m \\u001b[38;5;28;01mexcept\\u001b[39;00m (\\u001b[38;5;167;01mRuntimeError\\u001b[39;00m, \\u001b[38;5;167;01mTypeError\\u001b[39;00m, \\u001b[38;5;167;01mAttributeError\\u001b[39;00m) \\u001b[38;5;28;01mas\\u001b[39;00m e:\\n\\u001b[32m    166\\u001b[39m     warnings.warn(\\n\\u001b[32m    167\\u001b[39m         \\u001b[33mf\\u001b[39m\\u001b[33m\\\"\\u001b[39m\\u001b[33mCould not deserialize object \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00mdata_key\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[33m. \\u001b[39m\\u001b[33m\\\"\\u001b[39m\\n\\u001b[32m    168\\u001b[39m         \\u001b[33m\\\"\\u001b[39m\\u001b[33mConsider using `custom_objects` argument to replace \\u001b[39m\\u001b[33m\\\"\\u001b[39m\\n\\u001b[32m    169\\u001b[39m         \\u001b[33m\\\"\\u001b[39m\\u001b[33mthis object.\\u001b[39m\\u001b[38;5;130;01m\\\\n\\u001b[39;00m\\u001b[33m\\\"\\u001b[39m\\n\\u001b[32m    170\\u001b[39m         \\u001b[33mf\\u001b[39m\\u001b[33m\\\"\\u001b[39m\\u001b[33mException: \\u001b[39m\\u001b[38;5;132;01m{\\u001b[39;00me\\u001b[38;5;132;01m}\\u001b[39;00m\\u001b[33m\\\"\\u001b[39m\\n\\u001b[32m    171\\u001b[39m     )\\n\",\n",
    "      \"\\u001b[31mModuleNotFoundError\\u001b[39m: No module named 'gym'\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from huggingface_sb3 import load_from_hub\\n\",\n",
    "    \"repo_id = \\\"Classroom-workshop/assignment2-omar\\\" # The repo_id\\n\",\n",
    "    \"filename = \\\"ppo-LunarLander-v2.zip\\\" # The model filename.zip\\n\",\n",
    "    \"\\n\",\n",
    "    \"# When the model was trained on Python 3.8 the pickle protocol is 5\\n\",\n",
    "    \"# But Python 3.6, 3.7 use protocol 4\\n\",\n",
    "    \"# In order to get compatibility we need to:\\n\",\n",
    "    \"# 1. Install pickle5 (we done it at the beginning of the colab)\\n\",\n",
    "    \"# 2. Create a custom empty object we pass as parameter to PPO.load()\\n\",\n",
    "    \"custom_objects = {\\n\",\n",
    "    \"            \\\"learning_rate\\\": 0.0,\\n\",\n",
    "    \"            \\\"lr_schedule\\\": lambda _: 0.0,\\n\",\n",
    "    \"            \\\"clip_range\\\": lambda _: 0.0,\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"checkpoint = load_from_hub(repo_id, filename)\\n\",\n",
    "    \"model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"Fs0Y-qgPgLUf\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Let's evaluate this agent:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 15,\n",
    "   \"metadata\": {\n",
    "    \"id\": \"PAEVwK-aahfx\"\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"mean_reward=258.01 +/- 20.08413551389896\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"#@title\\n\",\n",
    "    \"eval_env = Monitor(gym.make(\\\"LunarLander-v2\\\"))\\n\",\n",
    "    \"mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\\n\",\n",
    "    \"print(f\\\"mean_reward={mean_reward:.2f} +/- {std_reward}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BQAwLnYFPk-s\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"## Some additional challenges üèÜ\\n\",\n",
    "    \"The best way to learn **is to try things by your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!\\n\",\n",
    "    \"\\n\",\n",
    "    \"In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?\\n\",\n",
    "    \"\\n\",\n",
    "    \"Here are some ideas to achieve so:\\n\",\n",
    "    \"* Train more steps\\n\",\n",
    "    \"* Try different hyperparameters for `PPO`. You can see them at https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters.\\n\",\n",
    "    \"* Check the [Stable-Baselines3 documentation](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) and try another model such as DQN.\\n\",\n",
    "    \"* **Push your new trained model** on the Hub üî•\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Compare the results of your LunarLander-v2 with your classmates** using the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) üèÜ\\n\",\n",
    "    \"\\n\",\n",
    "    \"Is moon landing too boring for you? Try to **change the environment**, why not use MountainCar-v0, CartPole-v1 or CarRacing-v0? Check how they work [using the gym documentation](https://www.gymlibrary.dev/) and have fun üéâ.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"9lM95-dvmif8\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"________________________________________________________________________\\n\",\n",
    "    \"Congrats on finishing this chapter! That was the biggest one, **and there was a lot of information.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**\\n\",\n",
    "    \"\\n\",\n",
    "    \"Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and have a solid foundations.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Naturally, during the course, we‚Äôre going to dive deeper into these concepts but **it‚Äôs better to have a good understanding of them now before diving into the next chapters.**\\n\",\n",
    "    \"\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {\n",
    "    \"id\": \"BjLhT70TEZIn\"\n",
    "   },\n",
    "   \"source\": [\n",
    "    \"Next time, in the bonus unit 1, you'll train Huggy the Dog to fetch the stick.\\n\",\n",
    "    \"\\n\",\n",
    "    \"<img src=\\\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\\\" alt=\\\"Huggy\\\"/>\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Keep learning, stay awesome ü§ó\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"accelerator\": \"GPU\",\n",
    "  \"colab\": {\n",
    "   \"collapsed_sections\": [\n",
    "    \"QAN7B0_HCVZC\",\n",
    "    \"BqPKw3jt_pG5\"\n",
    "   ],\n",
    "   \"private_outputs\": true,\n",
    "   \"provenance\": [],\n",
    "  \"include_colab_link\": true},\n",
    "  \"gpuClass\": \"standard\",\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.11.12\"\n",
    "  },\n",
    "  \"vscode\": {\n",
    "   \"interpreter\": {\n",
    "    \"hash\": \"ed7f8024e43d3b8f5ca3c5e1a8151ab4d136b3ecee1e3fd59e0766ccc55e1b10\"\n",
    "   }\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ],
   "id": "71422c02b193c454"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
